{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf.keras.applications import ResNet50\n",
    "from tf.keras.models import Model, Sequential\n",
    "from tf.keras.layers import Dense, GlobalAveragePooling2D, Embedding, LSTM\n",
    "from tf.keras.losses import CategoricalCrossEntropy\n",
    "from tf.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "num_epochs = 50 # not sure tbh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building the model:**\n",
    "## CNN: ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained ResNet on ImageNet dataset\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape= input_shape)\n",
    "\n",
    "# Freezing the layers\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "features = resnet_model.output\n",
    "\n",
    "FC_layer = GlobalAveragePooling2D()(features)\n",
    "FC_layer = Dense(256, activation = 'relu')(FC_layer)\n",
    "\n",
    "CNN_model = Model(inputs = resnet_model.input, output = FC_layer)\n",
    "\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = Sequential()\n",
    "\n",
    "# Rana's part returns ----> vocabulary_size, embedding_dim, max_sequence_length\n",
    "LSTM_model.add(Embedding(vocabulary_size, embedding_dim, input_length = max_sequence_length))\n",
    "LSTM_model.add(LSTM(256))\n",
    "LSTM_model.add(Dense(vocabulary_size, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = CategoricalCrossEntropy()\n",
    "optimizer = Adam()\n",
    "\n",
    "LSTM_model.compile(loss = loss_function, optimizer = optimizer)\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying teacher forcing \n",
    "# Note: This part is totally predicted to not perform as demanded yet\n",
    "# since the data preprocessing and captions are not available yet ==> RANAAAAAAAA\n",
    "\n",
    "# From Rana's part ---> input_sequences, target_sequences, to_one_hot_encoding(function)\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(input_sequences)):\n",
    "        hidden_state = LSTM_model.reset_states()\n",
    "        \n",
    "        for t in range(len(input_sequences[i])):\n",
    "            input_word = input_sequences[i][t]\n",
    "            target_word = target_sequences[i][t]\n",
    "            \n",
    "            input_word_encoded = to_one_hot_encoding(input_word, vocabulary_size)\n",
    "            \n",
    "            input_word_reshaped = input_word_encoded.reshape(1, 1, vocabulary_size)\n",
    "            \n",
    "            LSTM_output = LSTM_model(input_word_reshaped)\n",
    "            \n",
    "            loss = loss_function(target_word, LSTM_output)\n",
    "            \n",
    "            LSTM_model.train_on_batch(input_word_reshaped, target_word)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
